{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdd2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776cafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_experiments import run_experiment_with_custom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5bbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key already configured\n",
      "‚úÖ LLaMA API key already configured\n",
      "‚úÖ DeepSeek API key already configured\n",
      "\n",
      "üí° If all set, you can run experiments with any configured APIs!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üîê API Keys Setup - Set your API keys here (optional if already configured)\n",
    "# =============================================================================\n",
    "\n",
    "# Set your API keys here (leave empty if already configured in .env or environment)\n",
    "OPENAI_API_KEY = ''    # Your OpenAI API key\n",
    "LLAMA_API_KEY = ''     # Your LLaMA API key  \n",
    "DEEPSEEK_API_KEY = ''  # Your DeepSeek API key\n",
    "\n",
    "# Apply the keys to environment variables (only if not already set)\n",
    "import os\n",
    "\n",
    "# Check and set OpenAI key\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    print(\"‚úÖ OpenAI API key set from notebook\")\n",
    "elif os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"‚úÖ OpenAI API key already configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  OpenAI API key not available\")\n",
    "\n",
    "# Check and set LLaMA key\n",
    "if LLAMA_API_KEY:\n",
    "    os.environ['LLAMA_API_KEY'] = LLAMA_API_KEY\n",
    "    print(\"‚úÖ LLaMA API key set from notebook\")\n",
    "elif os.getenv('LLAMA_API_KEY'):\n",
    "    print(\"‚úÖ LLaMA API key already configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LLaMA API key not available\")\n",
    "\n",
    "# Check and set DeepSeek key\n",
    "if DEEPSEEK_API_KEY:\n",
    "    os.environ['DEEPSEEK_API_KEY'] = DEEPSEEK_API_KEY\n",
    "    print(\"‚úÖ DeepSeek API key set from notebook\")\n",
    "elif os.getenv('DEEPSEEK_API_KEY'):\n",
    "    print(\"‚úÖ DeepSeek API key already configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DeepSeek API key not available\")\n",
    "\n",
    "print(\"\\nüí° If all set, you can run experiments with any configured APIs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905485fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with llama3.1-70b using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/real_data/my_data.csv\n",
      " Loaded 9 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Sentiment_XL'\n",
      " Label distribution: {'Negative': 5, 'Positive': 3, 'Neutral': 1}\n",
      "\n",
      " Running experiment: llama3.1-70b_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:11<00:00,  1.23s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-70b using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-70b_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:13<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "                   Experiment  Accuracy  F1_Macro  Response_Rate\n",
      "llama3.1-70b_zero_shot_prompt       1.0       1.0            1.0\n",
      " llama3.1-70b_few_shot_prompt       1.0       1.0            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['llama3.1-70b_zero_shot_prompt', 'llama3.1-70b_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"llama3.1-70b\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429e144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with llama3.1-70b using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/real_data/my_data.csv\n",
      " Loaded 9 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Sentiment_XL'\n",
      " Label distribution: {'Negative': 5, 'Positive': 3, 'Neutral': 1}\n",
      "\n",
      " Running experiment: llama3.1-70b_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:12<00:00,  1.34s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-70b using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-70b_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:10<00:00,  1.22s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-405b using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-405b_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-405b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:13<00:00,  1.52s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-405b using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-405b_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 0.889, F1: 0.852, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-405b_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-405b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:27<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-405b_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "                    Experiment  Accuracy  F1_Macro  Response_Rate\n",
      " llama3.1-70b_zero_shot_prompt     1.000     1.000            1.0\n",
      "  llama3.1-70b_few_shot_prompt     1.000     1.000            1.0\n",
      "llama3.1-405b_zero_shot_prompt     0.889     0.852            1.0\n",
      " llama3.1-405b_few_shot_prompt     1.000     1.000            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['llama3.1-70b_zero_shot_prompt', 'llama3.1-70b_few_shot_prompt', 'llama3.1-405b_zero_shot_prompt', 'llama3.1-405b_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"llama3.1-70b\",\"llama3.1-405b\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c6c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with deepseek-chat using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/real_data/my_data.csv\n",
      " Loaded 9 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Sentiment_XL'\n",
      " Label distribution: {'Negative': 5, 'Positive': 3, 'Neutral': 1}\n",
      "\n",
      " Running experiment: deepseek-chat_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with deepseek-chat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:20<00:00,  8.99s/it]\n",
      "INFO:run_experiments:Running experiment with deepseek-chat using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/deepseek-chat_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: deepseek-chat_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with deepseek-chat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:46<00:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/deepseek-chat_few_shot_prompt_predictions.csv\n",
      " Accuracy: 0.889, F1: 0.619, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "                    Experiment  Accuracy  F1_Macro  Response_Rate\n",
      "deepseek-chat_zero_shot_prompt     1.000     1.000            1.0\n",
      " deepseek-chat_few_shot_prompt     0.889     0.619            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['deepseek-chat_zero_shot_prompt', 'deepseek-chat_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"deepseek-chat\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a264b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with gpt-4.1-mini using zero_shot_prompt\n",
      "Processing posts with gpt-4.1-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:05<00:00,  1.50it/s]\n",
      "INFO:run_experiments:Running experiment with gpt-4.1-mini using few_shot_prompt\n",
      "Processing posts with gpt-4.1-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gpt-4.1-mini_zero_shot_prompt', 'gpt-4.1-mini_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"gpt-4.1-mini\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4092af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with o3-mini using zero_shot_prompt\n",
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:28<00:00,  3.14s/it]\n",
      "INFO:run_experiments:Running experiment with o3-mini using few_shot_prompt\n",
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:32<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['o3-mini_zero_shot_prompt', 'o3-mini_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"o3-mini\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a054edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with o3-mini using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/sample_data/sample_posts_test.csv\n",
      " Loaded 5 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Expert_Label'\n",
      " Label distribution: {'Neutral': 3, 'Positive': 2}\n",
      "\n",
      " Running experiment: o3-mini_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:09<00:00,  1.86s/it]\n",
      "INFO:run_experiments:Running experiment with o3-mini using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/o3-mini_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: o3-mini_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/o3-mini_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "              Experiment  Accuracy  F1_Macro  Response_Rate\n",
      "o3-mini_zero_shot_prompt       1.0       1.0            1.0\n",
      " o3-mini_few_shot_prompt       1.0       1.0            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['o3-mini_zero_shot_prompt', 'o3-mini_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/sample_data/sample_posts_test.csv\", \n",
    "    models=[\"o3-mini\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Expert_Label\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c02f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
