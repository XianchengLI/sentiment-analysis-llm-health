{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdd2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776cafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5bbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key already configured\n",
      "‚úÖ LLaMA API key already configured\n",
      "‚úÖ DeepSeek API key already configured\n",
      "\n",
      "üí° If all set, you can run experiments with any configured APIs!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üîê API Keys Setup - Set your API keys here (optional if already configured)\n",
    "# =============================================================================\n",
    "\n",
    "# Set your API keys here (leave empty if already configured in .env or environment)\n",
    "OPENAI_API_KEY = ''    # Your OpenAI API key\n",
    "LLAMA_API_KEY = ''     # Your LLaMA API key  \n",
    "DEEPSEEK_API_KEY = ''  # Your DeepSeek API key\n",
    "\n",
    "# Apply the keys to environment variables (only if not already set)\n",
    "import os\n",
    "\n",
    "# Check and set OpenAI key\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    print(\"‚úÖ OpenAI API key set from notebook\")\n",
    "elif os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"‚úÖ OpenAI API key already configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  OpenAI API key not available\")\n",
    "\n",
    "# Check and set LLaMA key\n",
    "if LLAMA_API_KEY:\n",
    "    os.environ['LLAMA_API_KEY'] = LLAMA_API_KEY\n",
    "    print(\"‚úÖ LLaMA API key set from notebook\")\n",
    "elif os.getenv('LLAMA_API_KEY'):\n",
    "    print(\"‚úÖ LLaMA API key already configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LLaMA API key not available\")\n",
    "\n",
    "# Check and set DeepSeek key\n",
    "if DEEPSEEK_API_KEY:\n",
    "    os.environ['DEEPSEEK_API_KEY'] = DEEPSEEK_API_KEY\n",
    "    print(\"‚úÖ DeepSeek API key set from notebook\")\n",
    "elif os.getenv('DEEPSEEK_API_KEY'):\n",
    "    print(\"‚úÖ DeepSeek API key already configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DeepSeek API key not available\")\n",
    "\n",
    "print(\"\\nüí° If all set, you can run experiments with any configured APIs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e49bf",
   "metadata": {},
   "source": [
    "### compare LLM results with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a054edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with o3-mini using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/sample_data/sample_posts_test.csv\n",
      " Loaded 5 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Expert_Label'\n",
      " Label distribution: {'Neutral': 3, 'Positive': 2}\n",
      "\n",
      " Running experiment: o3-mini_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:09<00:00,  1.86s/it]\n",
      "INFO:run_experiments:Running experiment with o3-mini using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/o3-mini_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: o3-mini_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/o3-mini_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "              Experiment  Accuracy  F1_Macro  Response_Rate\n",
      "o3-mini_zero_shot_prompt       1.0       1.0            1.0\n",
      " o3-mini_few_shot_prompt       1.0       1.0            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['o3-mini_zero_shot_prompt', 'o3-mini_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/sample_data/sample_posts_test.csv\", \n",
    "    models=[\"o3-mini\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Expert_Label\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb72897",
   "metadata": {},
   "source": [
    "### only prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ece8796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/sample_data/sample_posts_test.csv\n",
      " Loaded 5 posts for prediction\n",
      " Using columns: PostID='PostId', Content='Body'\n",
      " Predicting sentiment using prompt: few_shot_prompt\n",
      "\n",
      " Predicting with gpt-4o-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with gpt-4o-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gpt-4o-mini: 5/5 predictions (100.0% success rate)\n",
      " Distribution: {'Neutral': 3, 'Positive': 2}\n",
      "\n",
      " Predicting with llama3.1-70b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:06<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " llama3.1-70b: 5/5 predictions (100.0% success rate)\n",
      " Distribution: {'Neutral': 3, 'Positive': 2}\n",
      "\n",
      " Predictions saved to: ../results/predictions/sentiment_predictions_20250715_015718.csv\n",
      " Results summary:\n",
      "   - Total posts: 5\n",
      "   - Models used: ['gpt-4o-mini', 'llama3.1-70b']\n",
      "   - New columns: ['Predicted_gpt-4o-mini', 'Predicted_llama3.1-70b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = predict_sentiment_batch(\n",
    "    data_path=\"../data/sample_data/sample_posts_test.csv\",\n",
    "    models=[\"gpt-4o-mini\", \"llama3.1-70b\"],\n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08498e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostId</th>\n",
       "      <th>Body</th>\n",
       "      <th>Expert_Label</th>\n",
       "      <th>Category</th>\n",
       "      <th>Predicted_gpt-4o-mini</th>\n",
       "      <th>Predicted_llama3.1-70b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POST_001</td>\n",
       "      <td>I thought I'd need help last night, but I mana...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Health Improvement</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POST_018</td>\n",
       "      <td>The research shows mixed results for this trea...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Generated</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POST_016</td>\n",
       "      <td>Has anyone tried the new inhaler device? Wonde...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Generated</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POST_002</td>\n",
       "      <td>There was a fuss about the drug about ten year...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Uncertainty</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POST_009</td>\n",
       "      <td>The new inhaler technique really helped me dur...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Generated</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PostId                                               Body Expert_Label  \\\n",
       "0  POST_001  I thought I'd need help last night, but I mana...     Positive   \n",
       "1  POST_018  The research shows mixed results for this trea...      Neutral   \n",
       "2  POST_016  Has anyone tried the new inhaler device? Wonde...      Neutral   \n",
       "3  POST_002  There was a fuss about the drug about ten year...      Neutral   \n",
       "4  POST_009  The new inhaler technique really helped me dur...     Positive   \n",
       "\n",
       "             Category Predicted_gpt-4o-mini Predicted_llama3.1-70b  \n",
       "0  Health Improvement              Positive               Positive  \n",
       "1           Generated               Neutral                Neutral  \n",
       "2           Generated               Neutral                Neutral  \n",
       "3         Uncertainty               Neutral                Neutral  \n",
       "4           Generated              Positive               Positive  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67417c6e",
   "metadata": {},
   "source": [
    "### Run with real data\n",
    "#### Before using with real data, make sure you saved data in the folder: real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905485fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with llama3.1-70b using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/real_data/my_data.csv\n",
      " Loaded 9 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Sentiment_XL'\n",
      " Label distribution: {'Negative': 5, 'Positive': 3, 'Neutral': 1}\n",
      "\n",
      " Running experiment: llama3.1-70b_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:11<00:00,  1.23s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-70b using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-70b_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:13<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "                   Experiment  Accuracy  F1_Macro  Response_Rate\n",
      "llama3.1-70b_zero_shot_prompt       1.0       1.0            1.0\n",
      " llama3.1-70b_few_shot_prompt       1.0       1.0            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['llama3.1-70b_zero_shot_prompt', 'llama3.1-70b_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", # change file name\n",
    "    models=[\"llama3.1-70b\"],  \n",
    "    post_id_col=\"PostId\",# change column names\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429e144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with llama3.1-70b using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/real_data/my_data.csv\n",
      " Loaded 9 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Sentiment_XL'\n",
      " Label distribution: {'Negative': 5, 'Positive': 3, 'Neutral': 1}\n",
      "\n",
      " Running experiment: llama3.1-70b_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:12<00:00,  1.34s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-70b using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-70b_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-70b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:10<00:00,  1.22s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-405b using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-70b_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-405b_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-405b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:13<00:00,  1.52s/it]\n",
      "INFO:run_experiments:Running experiment with llama3.1-405b using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-405b_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 0.889, F1: 0.852, Response Rate: 1.000\n",
      "\n",
      " Running experiment: llama3.1-405b_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with llama3.1-405b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:27<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/llama3.1-405b_few_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "                    Experiment  Accuracy  F1_Macro  Response_Rate\n",
      " llama3.1-70b_zero_shot_prompt     1.000     1.000            1.0\n",
      "  llama3.1-70b_few_shot_prompt     1.000     1.000            1.0\n",
      "llama3.1-405b_zero_shot_prompt     0.889     0.852            1.0\n",
      " llama3.1-405b_few_shot_prompt     1.000     1.000            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['llama3.1-70b_zero_shot_prompt', 'llama3.1-70b_few_shot_prompt', 'llama3.1-405b_zero_shot_prompt', 'llama3.1-405b_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"llama3.1-70b\",\"llama3.1-405b\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c6c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with deepseek-chat using zero_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data from: ../data/real_data/my_data.csv\n",
      " Loaded 9 posts\n",
      " Using columns: PostID='PostId', Content='Body', Label='Sentiment_XL'\n",
      " Label distribution: {'Negative': 5, 'Positive': 3, 'Neutral': 1}\n",
      "\n",
      " Running experiment: deepseek-chat_zero_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with deepseek-chat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:20<00:00,  8.99s/it]\n",
      "INFO:run_experiments:Running experiment with deepseek-chat using few_shot_prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/deepseek-chat_zero_shot_prompt_predictions.csv\n",
      " Accuracy: 1.000, F1: 1.000, Response Rate: 1.000\n",
      "\n",
      " Running experiment: deepseek-chat_few_shot_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing posts with deepseek-chat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:46<00:00,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results saved to: ../results/custom_experiments/deepseek-chat_few_shot_prompt_predictions.csv\n",
      " Accuracy: 0.889, F1: 0.619, Response Rate: 1.000\n",
      "\n",
      " Generating comparison summary...\n",
      " Comparison summary saved to: ../results/custom_experiments/comparison_summary.csv\n",
      "\n",
      " Results Summary:\n",
      "                    Experiment  Accuracy  F1_Macro  Response_Rate\n",
      "deepseek-chat_zero_shot_prompt     1.000     1.000            1.0\n",
      " deepseek-chat_few_shot_prompt     0.889     0.619            1.0\n",
      "\n",
      " Experiment completed! Results saved to: ../results/custom_experiments/\n",
      "dict_keys(['deepseek-chat_zero_shot_prompt', 'deepseek-chat_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"deepseek-chat\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\"\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a264b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with gpt-4.1-mini using zero_shot_prompt\n",
      "Processing posts with gpt-4.1-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:05<00:00,  1.50it/s]\n",
      "INFO:run_experiments:Running experiment with gpt-4.1-mini using few_shot_prompt\n",
      "Processing posts with gpt-4.1-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gpt-4.1-mini_zero_shot_prompt', 'gpt-4.1-mini_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"gpt-4.1-mini\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4092af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiments:Running experiment with o3-mini using zero_shot_prompt\n",
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:28<00:00,  3.14s/it]\n",
      "INFO:run_experiments:Running experiment with o3-mini using few_shot_prompt\n",
      "Processing posts with o3-mini: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:32<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['o3-mini_zero_shot_prompt', 'o3-mini_few_shot_prompt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment_with_custom_data(\n",
    "    data_path=\"../data/real_data/my_data.csv\", \n",
    "    models=[\"o3-mini\"],  \n",
    "    post_id_col=\"PostId\",\n",
    "    content_col=\"Body\", \n",
    "    expert_label_col=\"Sentiment_XL\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c02f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
